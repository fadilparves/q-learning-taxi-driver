{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import math\n",
    "from matplotlib import rc\n",
    "import sys\n",
    "from contextlib import closing\n",
    "from io import StringIO\n",
    "from gym import utils\n",
    "from gym.envs.toy_text import discrete\n",
    "from gym.envs.registration import register\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.3)\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP = [\n",
    "    \"+---------+\",\n",
    "    \"|R: :A: :G|\",\n",
    "    \"| : : : : |\",\n",
    "    \"| :A:A: : |\",\n",
    "    \"| : : : : |\",\n",
    "    \"|Y: :A:B: |\",\n",
    "    \"+---------+\",\n",
    "]\n",
    "\n",
    "\n",
    "class SafeCab(discrete.DiscreteEnv):\n",
    "    \"\"\"\n",
    "    Modified Taxi Problem\n",
    "    from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\"\n",
    "    by Tom Dietterich\n",
    "\n",
    "    Description:\n",
    "    There are four designated locations in the grid world indicated by R(ed), B(lue), G(reen), and Y(ellow). \n",
    "    When the episode starts, the taxi starts off at a random square and the passenger is at a random location. \n",
    "    The taxi drive to the passenger's location, pick up the passenger, \n",
    "    drive to the passenger's destination (another one of the four specified locations), and then drop off the passenger. \n",
    "    The episode ends if the taxi ends up on anomaly location. Once the passenger is dropped off, the episode ends.\n",
    "\n",
    "    Observations: \n",
    "    There are 500 discrete states since there are 25 taxi positions, \n",
    "    5 possible locations of the passenger (including the case when the passenger is the taxi), \n",
    "    and 4 destination locations. \n",
    "    \n",
    "    Actions: \n",
    "    There are 6 discrete deterministic actions:\n",
    "    - 0: move south\n",
    "    - 1: move north\n",
    "    - 2: move east \n",
    "    - 3: move west \n",
    "    - 4: pickup passenger\n",
    "    - 5: dropoff passenger\n",
    "    \n",
    "    Rewards: \n",
    "    There is a reward of -1 for each action and an additional reward of +1000 for delievering the passenger. \n",
    "    There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally. \n",
    "    Interaction with an anomaly gives -1000 reward and end the episode.\n",
    "\n",
    "    Rendering:\n",
    "    - blue: passenger\n",
    "    - magenta: destination\n",
    "    - yellow: empty taxi\n",
    "    - green: full taxi\n",
    "    - red: anomaly\n",
    "    - other letters (R, G, B and Y): locations for passengers and destinations\n",
    "\n",
    "    actions:\n",
    "    - 0: south\n",
    "    - 1: north\n",
    "    - 2: east\n",
    "    - 3: west\n",
    "    - 4: pickup\n",
    "    - 5: dropoff\n",
    "\n",
    "    state space is represented by:\n",
    "        (taxi_row, taxi_col, passenger_location, destination)\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.desc = np.asarray(MAP, dtype='c')\n",
    "\n",
    "        self.locs = locs = [(0,0), (0,4), (4,0), (4,3)]\n",
    "        self.anomaly_locs = [(0,2), (2,1), (2,2), (4,2)]\n",
    "\n",
    "        num_states = 500\n",
    "        num_rows = 5\n",
    "        num_columns = 5\n",
    "        max_row = num_rows - 1\n",
    "        max_col = num_columns - 1\n",
    "        initial_state_distrib = np.zeros(num_states)\n",
    "        num_actions = 6\n",
    "        P = {state: {action: []\n",
    "                     for action in range(num_actions)} for state in range(num_states)}\n",
    "        for row in range(num_rows):\n",
    "            for col in range(num_columns):\n",
    "                for pass_idx in range(len(locs) + 1):  # +1 for being inside taxi\n",
    "                    for dest_idx in range(len(locs)):\n",
    "                        state = self.encode(row, col, pass_idx, dest_idx)\n",
    "                        if pass_idx < 4 and pass_idx != dest_idx:\n",
    "                            initial_state_distrib[state] += 1\n",
    "                        for action in range(num_actions):\n",
    "                            # defaults\n",
    "                            new_row, new_col, new_pass_idx = row, col, pass_idx\n",
    "                            reward = -1 # default reward when there is no pickup/dropoff\n",
    "                            done = False\n",
    "                            taxi_loc = (row, col)\n",
    "\n",
    "                            if action == 0:\n",
    "                                new_row = min(row + 1, max_row)\n",
    "                            elif action == 1:\n",
    "                                new_row = max(row - 1, 0)\n",
    "                            if action == 2:\n",
    "                                new_col = min(col + 1, max_col)\n",
    "                            elif action == 3:\n",
    "                                new_col = max(col - 1, 0)\n",
    "                            elif action == 4:  # pickup\n",
    "                                if (pass_idx < 4 and taxi_loc == locs[pass_idx]):\n",
    "                                    new_pass_idx = 4\n",
    "                                else: # passenger not at location\n",
    "                                    reward = -10\n",
    "                            elif action == 5:  # dropoff\n",
    "                                if (taxi_loc == locs[dest_idx]) and pass_idx == 4:\n",
    "                                    new_pass_idx = dest_idx\n",
    "                                    done = True\n",
    "                                    reward = 1000\n",
    "                                elif (taxi_loc in locs) and pass_idx == 4:\n",
    "                                    new_pass_idx = locs.index(taxi_loc)\n",
    "                                else: # dropoff at wrong location\n",
    "                                    reward = -10\n",
    "                                    \n",
    "                            new_loc = (new_row, new_col)\n",
    "                            if new_loc in self.anomaly_locs:\n",
    "                                reward = -1000\n",
    "                                done = True\n",
    "                                \n",
    "                            new_state = self.encode(new_row, new_col, new_pass_idx, dest_idx)\n",
    "                            P[state][action].append((1.0, new_state, reward, done))\n",
    "                            \n",
    "        initial_state_distrib /= initial_state_distrib.sum()\n",
    "        discrete.DiscreteEnv.__init__(self, num_states, num_actions, P, initial_state_distrib)\n",
    "\n",
    "    def encode(self, taxi_row, taxi_col, pass_loc, dest_idx):\n",
    "        # (5) 5, 5, 4\n",
    "        i = taxi_row\n",
    "        i *= 5\n",
    "        i += taxi_col\n",
    "        i *= 5\n",
    "        i += pass_loc\n",
    "        i *= 4\n",
    "        i += dest_idx\n",
    "        return i\n",
    "\n",
    "    def decode(self, i):\n",
    "        out = []\n",
    "        out.append(i % 4)\n",
    "        i = i // 4\n",
    "        out.append(i % 5)\n",
    "        i = i // 5\n",
    "        out.append(i % 5)\n",
    "        i = i // 5\n",
    "        out.append(i)\n",
    "        assert 0 <= i < 5\n",
    "        return reversed(out)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "        out = self.desc.copy().tolist()\n",
    "        out = [[c.decode('utf-8') for c in line] for line in out]\n",
    "        taxi_row, taxi_col, pass_idx, dest_idx = self.decode(self.s)\n",
    "        \n",
    "        def ul(x): return \"_\" if x == \" \" else x\n",
    "        \n",
    "        if pass_idx < 4:\n",
    "            out[1 + taxi_row][2 * taxi_col + 1] = utils.colorize(\n",
    "                out[1 + taxi_row][2 * taxi_col + 1], 'yellow', highlight=True)\n",
    "            pi, pj = self.locs[pass_idx]\n",
    "            out[1 + pi][2 * pj + 1] = utils.colorize(out[1 + pi][2 * pj + 1], 'blue', bold=True)\n",
    "        else:  # passenger in taxi\n",
    "            out[1 + taxi_row][2 * taxi_col + 1] = utils.colorize(\n",
    "                ul(out[1 + taxi_row][2 * taxi_col + 1]), 'green', highlight=True)\n",
    "\n",
    "        di, dj = self.locs[dest_idx]\n",
    "        out[1 + di][2 * dj + 1] = utils.colorize(out[1 + di][2 * dj + 1], 'magenta')\n",
    "        \n",
    "        for (zx, zy) in self.anomaly_locs:\n",
    "          out[1 + zx][2 * zy + 1] = utils.colorize(\n",
    "            out[1 + zx][2 * zy + 1], 'red', bold=True)\n",
    "        \n",
    "        outfile.write(\"\\n\".join([\"\".join(row) for row in out]) + \"\\n\")\n",
    "        if self.lastaction is not None:\n",
    "            outfile.write(\"  ({})\\n\".format([\"South\", \"North\", \"East\", \"West\", \"Pickup\", \"Dropoff\"][self.lastaction]))\n",
    "        else: outfile.write(\"\\n\")\n",
    "\n",
    "        # No need to return anything for human\n",
    "        if mode != 'human':\n",
    "            with closing(outfile):\n",
    "                return outfile.getvalue()\n",
    "              \n",
    "register(\n",
    "    id='SafeCab-v0',\n",
    "    entry_point=f\"{__name__}:SafeCab\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Episode: {frame['episode']}\")\n",
    "        print(f\"Timestep: {frame['step']}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SafeCab-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: :\u001b[31;1mA\u001b[0m: :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| :\u001b[31;1mA\u001b[0m:\u001b[31;1mA\u001b[0m: : |\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "|Y: :\u001b[31;1mA\u001b[0m:\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, n_states, n_actions, decay_rate=0.0001, learning_rate=0.7, gamma=0.618):\n",
    "        self.n_actions = n_actions\n",
    "        self.q_table = np.zeros((n_states, n_actions))\n",
    "        self.epsilon = 1.0\n",
    "        self.max_epsilon = 1.0\n",
    "        self.min_epsilon = 0.01\n",
    "        self.decay_rate = decay_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma #discount rate\n",
    "        self.epsilons_ = []\n",
    "        \n",
    "    def choose_action(self, explore=True):\n",
    "        exploration_tradeoff = np.random.uniform(0,1)\n",
    "\n",
    "        if explore and exploration_tradeoff < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state, :])\n",
    "            \n",
    "        def learn(self, state, action, reward, next_state, done, episode):\n",
    "            self.q_table[state, action] = self.q_table[state, action] + self.learning_rate * (reward + self.gamma * np.max(self.q_table[new_state, :]) - self.q_table[state, action])\n",
    "\n",
    "            if done:\n",
    "                #reduce epsilon to decrease the explore over time\n",
    "                self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon) * np.exp(-self.decay_rate * episode)\n",
    "                self.epsilons_.append(self.epsilon)\n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 60000\n",
    "total_test_episodes = 10\n",
    "\n",
    "agent = Agent(env.observation_space.n, env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Agent' object has no attribute 'choose_action'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d5078b6564c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         untrainded_frames.append({\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Agent' object has no attribute 'choose_action'"
     ]
    }
   ],
   "source": [
    "untrainded_frames = []\n",
    "\n",
    "for episode in range(total_test_episodes):\n",
    "    state = env.reset()\n",
    "    step = 1\n",
    "    while True:\n",
    "        action = agent.choose_action()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        untrainded_frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'episode': episode + 1,\n",
    "            'step': step,\n",
    "            'reward' : reward\n",
    "        })\n",
    "        \n",
    "        if done:\n",
    "            step = 0\n",
    "            break\n",
    "        state = new_state\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
